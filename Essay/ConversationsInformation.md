#  Conversations about information
Craig Duncan
31 January 2020

## 

Conversations about infomation are productive, in that we can think some more about what we want to see in agreements or other legal documents that record information.

Even in the 20th century, the archaic legal drafting conventions in something like 'wills' represents something that embodies kinds of language, sentence structure and word formulas that is resistent to change.  

They use words and terminology that involve an unncessarily high cognitive burden for the average reader.   People have to decipher the odd language, then work out what happens when people 'attain' a certain age.   The lawyers write down the rules and generalisations because it can help deal with uncertain futures (e.g. future children or grandchildren not yet known).  

## Concrete representations

These formulas (lawyers might call it 'machinery') take on a life of their own: the document itself has to be read, interpreted and then the formulas applied to whatever the facts happen to be.  But for a given set of hypothetical facts, the formulas can resolve themselves in a certain state of affairs, which can be represented in a particular way.  More direct, concrete representations could be written down and laid out, to distinguish the main differences.   The retention of multiple combinatorial rules is problematic because it postpones 'testing' the outcome of agreements.   A simulation of actual events is preferred as often as possible.

# Spatial representation

The human mind is always trying to make use of spatial information, because it can very efficiently encode not only the relevant objects, but their relationships to one another.  We are repurposing a very efficient form of information (which is multi-dimensional, not sequential and linear).

Simple lines and axis in mathematics imply relative size, position even if we do not see the exact coordinates.

We can talk about representations that are direct, visual (spatial), and have conversations about them, which skip to the implications or opinions we have about them, and are less directed to first describing *what* we are talking about.

Ordinary writing has a visual element - letters follow conventions, and there is a spatial layout within them (not just sequence, but constraints as to how much of the page they follow).  A legal document is the same - with 'topics' usually given headings.   

# Case examples

## Legal Definitions

Definitions are put into a separate section in most legal documents because there is an implicit acknowledgement they are a different data structure - a way of 'looking up' information, but the exact reasons or preference for this convention are not discussed openly, or with any attempt to be scientific or consider design implications.   

Clients still complain about how much to-and-fro there is when negotiating a legal agreement and having to get the definitions correct.   The absence of a suitable technology that can handle some of the spatial formatting is apparent if you start to think about ways of avoiding these 'clerical' tasks.   

## Data-orientated approach

We could start to think about relying on the prior designation of definitions as a kind of data, perhaps a kind of data that is brought into the document with a specific clause - so it is bound to the source, and it will then be able to be toggled on/off if the clause is removed.  These data structures can only be introduced if there is a concerted effort to preparing software that can handle text data types as content. 

What do I mean by 'text data types as content'?  People think legal tech is inserting variables (facts mostly) into 'templates'.   But legal text in the main document is really the main data.  

Legal text has different data types, and underutilised opportunities to manipulate it.  People are still playing around at the edges (and have not independently learned about the data, but have been heavily influenced by software conventions like mail merge that flow from the limitations of word processing)

## Spatial elements and human visual system

Things like lists are not merely abstract mental structures, but when we also write them as a vertical 'ladder' it reflects our visual system of processing as well.  We rarely process information in just one way.

Even in written text, the mind can quickly assess different data types visually, and this has encouraged conventions about putting different types of data (e.g. definitions) in **proximity** to one another (like-with-like), as a **vertical list, sorted** alphabetically.   

These are design and visual system conventions, also motivated by efficiency.  "Reading", for the sighted, is never just mentally 'hearing' the words.

When drafting, the concern has often been to arrange information in a particular way, to reduce some of the cognitive load for the readers, or because convention dictates it.   But the extent to which we are aware of cognitive load, or reducing cognitive load, for the information that is being recorded, The extent to which we pay attention to reducing the burden of having to do this manual layout manually as well is also probably low.  

Word processors have been popular because they allow us to arrange information even more flexibly than we used to do with typewriters, that laid down letters in a sequential way, and relied on mostly perfect execution of laying out the characters one by one.  There was a defined process of preparing something in draft, then the process of getting it perfect for publication.  With a Word Processor, if we thought of a new clause as an afterthought, we could go back and insert definitions for it.  The distinction between draft and final became blurred.   Negotiations and changes became more fluid, but they were always about coming up with the finished document - a combination of how it read and how it looked.

For those who have a more data-orientated approach to what is laid out on a page or whiteboard, the focus on superficial layouts and text without data types has always seemed like an unnecessary burden.  Computers can also process textual information, but even more robustly and flexibly than Word Processors would lead us to believe.   They can store and index information in memory, sequencing it, and then provide rule-based algorithms to deal with it.    Computers are able to process both the data both as non-visual structures (which nonetheless have position and sequencing), but also to display it in a way that suits the human visual system/cognitive system.

Given that is the case, the focus within 'legaltech' of trying to create new templating systems seems like a step backward.  The frequent use of 'templates' in Word Processor formats seems odd, given that it sidesteps the opportunity to focus on the text content, not just external data.  It creates even more rigid text documents (even if some of the input data is gathered by 'interviews' or GUI data capture screens).   It is an even further restriction on the liberties that Word Processors seem to have given us.

Learning Management Systems offer schools and educational institutions a lot of information, organised in a careful way.  The same attention could be devoted to how a law firm, and the Court system, deals with information, but it must identify some objective, general truths about the kind of data that is contained in text documents.  This data is often written in a conversation or dialogue, and not necessarily intended as a monologue, even if it appears to be 'one document'.   That there may be one text, but many voices, is something that is very powerful in how we might design documents in professions that involve propositions, reflection, argument and agreement, albeit at different times.

What stops us paying attention to the implicit data types in text documents and making computers do even work on them for our benefit?  People can certainly work with information in a browser, and select options.  But when they are given a free-form writing platform they seem less data-aware.  Here are some speculative reasons:

1.  People are conditioned to give high value to the visual/spatial representation that is achieved in a computer (however much crafting is required).  They may, naively, assume that the computer can 'see' the same structures in the same way they do.  Perhaps they think that the only structure that matters is the one a human being can see.
2.  People may not even appreciate that these structures can be transformed, or represented in different ways, both electronically and visually.   We do not need to store information in exactly the same way as it is represented on-screen; and not all the time.
3.  People may not realise that even what they see may be produced by data structures of some sort, but not necessarily the same ones that they are using.  They may be data structures primarily for showing things the way humans do, but are not conducive for use in other ways.  

## Data-bias in Word Processor design

The programming languages or libraries that have grown up around Word Processors are heavily biased in favour of making the data fit the software application, rather than the software application fit the data, or maintaining the ability of the computer system, and other applications to work with the same data.  Some software applications, like Word Processors do seem to adopt object-orientated data structures that are primarily concerned with the software application that generates a visual representation designed to be read directly by humans, not manipulated.   It is a lazy assumption, in that:

1.  it is relying on the human capacity to interpret the information visually, and 
2.  it cannot be called an unadorned data format that is not dependent upon the software application that generated it.

Data structures like OOXML reflect these specific assumptions.  OOXML is not conducive to working with data types that are important to the user (and which may vary according to context), nor is it free of specific references to concepts related to the software application that works with it.  It is not sufficiently abstract that something like a 'style' can be efficiently generated in isolation, nor is it disassociated from its specific implementation in Word.  The Word "Document Object Model" is not some generic schema for information.  It prioritises attention on text layout.  It also does not abstract away specific documents so that sets of documents can be produced at one time.

When working with data, the way in which we are expected to prepare data for use with software applications is important.  It is possible for a particular software application to take a non-proprietary attitude to the data: to transform it but to keep an arm's-length distance from the input data.   This approach to design makes the software responsible for restructuring the data as and when needed to suit a new purpose, or to translate an information into a new form without making that new form the permanent input format for the program.  

Software that operates at arm's length for display purposes should not prevent the simultaneous ability to perform operations on the data structures, or to allow better data preparation to suit both manipulation of data filtered, sorted, merged, classified and exposed to different computational functions), as well as rendering or layout.   

Another consideration is whether the software application assists the user to record different states (invisibly, perhaps) so that it is even better prepared for these data manipulation functions.   If the software application is designed to be more than just a 'display' program, it could record information that will allow data manipulation first, and then to update the rendering second.

Like the MVC model, a design paradigm in which there is D-DS-V (Data, data structures, View)  ?  If they are, is this something built into the user interface (so that data and state is silently recorded), or is it something that requires post-processing?  We might also have data-data structures (conversations, threads etc) - View.
